{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather relevant data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape demographic data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks to confirm page exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://www.zip-codes.com{}'\n",
    "columbus_branch ='/city/oh-columbus.asp'\n",
    "# reinforces integrity of the data collection\n",
    "accessed = False\n",
    "while accessed == False:\n",
    "    try:\n",
    "        response = requests.get(url.format(columbus_branch))\n",
    "        print(response)\n",
    "        accessed = True\n",
    "    except:\n",
    "        accesssed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathers relevant data on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.table.tr.find_all('td')[1].find_all('table')[1]\n",
    "    print(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to help web scrape zip code information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Fix zip info extraction method.\n",
    "# utilize when using the table the [0-4] table \n",
    "# BeautifulSoup(response.text, 'html.parser').table.tr.find_all('td')[1].find_all('table')\n",
    "def zip_info(response):\n",
    "    page = BeautifulSoup(response.text, 'html.parser').find_all(\"table\", class_=\"statTable\")\n",
    "    #print(page)\n",
    "    dict = {}\n",
    "    for table in page:\n",
    "        for row in table.find_all('tr'):\n",
    "            dict[row.find_all('td')[0].string] = row.find_all('td')[1].string        \n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses through and gathers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "zip_info_df = pd.DataFrame()\n",
    "zip_url = '/zip-code/{}/zip-code-{}.asp'\n",
    "for row in tbl.find_all('tr'):\n",
    "    #checks for header\n",
    "    if row.find_all('td')[0].string != 'ZIP Code':\n",
    "        if row.find_all('td')[1].string == 'Standard':\n",
    "            zip_code = row.find_all('td')[0].string.strip(\" ZIPCode\")\n",
    "            print(zip_code)\n",
    "            #coded just to make sure that we can access the website\n",
    "            accessed = False\n",
    "            while accessed == False:\n",
    "                try:\n",
    "                    zip_response = requests.get(url.format(zip_url.format(zip_code,zip_code)))\n",
    "                    accessed = True\n",
    "                    i+=1\n",
    "                    zip_dict = zip_info(zip_response)\n",
    "                    zip_info_df = zip_info_df.append(zip_dict, ignore_index=True)\n",
    "                except:\n",
    "                    accessed = False\n",
    "print(i)\n",
    "print(zip_info_df)\n",
    "# reads in all zip code responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_info_df.dtypes\n",
    "zip_info_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location data for Hospitals and Pharmacies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests # library to handle requests\n",
    "import pandas as pd # library for data analsysis\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import random # library for random number generation\n",
    "\n",
    "# !conda install -c conda-forge geopy --yes \n",
    "from geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values\n",
    "\n",
    "# libraries for displaying images\n",
    "from IPython.display import Image \n",
    "from IPython.core.display import HTML \n",
    "    \n",
    "# tranforming json file into a pandas dataframe library\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# !conda install -c conda-forge folium=0.5.0 --yes\n",
    "import folium # plotting library\n",
    "\n",
    "print('Folium installed')\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials for foursquare_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your credentails:\n",
      "CLIENT_ID: 3PRTA5P0AY2A12Z51Z1UHCZH124K5AYLU4PGQHQ0BMCV5FWD\n",
      "CLIENT_SECRET:JOQYBGLRRR0QNOYE43VOVALU1DFGK4TGSR23SS3J53X2G4JW\n"
     ]
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "CLIENT_ID = '3PRTA5P0AY2A12Z51Z1UHCZH124K5AYLU4PGQHQ0BMCV5FWD' # your Foursquare ID\n",
    "CLIENT_SECRET = 'JOQYBGLRRR0QNOYE43VOVALU1DFGK4TGSR23SS3J53X2G4JW' # your Foursquare Secret\n",
    "VERSION = '20180604'\n",
    "LIMIT = 60\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information for using foursquare API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_info_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_IDs = {'Hospital':'4bf58dd8d48988d196941735', 'Pharmacy': '4bf58dd8d48988d10f951735'}\n",
    "geo_coords = zip_info_df[['Latitude:','Longitude:']]\n",
    "radius = 10000 #about 6.2 miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create parameters that are not hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&categoryId={}&v={}&radius={}&limit={}'\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabs data for all hospitals and pharmacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "# Hospital and Pharmacy Data Frame\n",
    "hp_df = pd.DataFrame()\n",
    "while i<len(geo_coords): \n",
    "    lat = geo_coords.iloc[i]['Latitude:']\n",
    "    long = geo_coords.iloc[i]['Longitude:']\n",
    "    #hospitals\n",
    "    temp_url = url.format(CLIENT_ID, CLIENT_SECRET, lat, long, CAT_IDs['Hospital'], VERSION, radius, LIMIT)\n",
    "    results = requests.get(temp_url).json()\n",
    "    hps = results['response']['venues']\n",
    "    hp_df=hp_df.append(json_normalize(hps), ignore_index=True)\n",
    "    #pharmacies\n",
    "    temp_url = url.format(CLIENT_ID, CLIENT_SECRET, lat, long, CAT_IDs['Pharmacy'], VERSION, radius, LIMIT)\n",
    "    results = requests.get(temp_url).json()\n",
    "    hps = results['response']['venues']\n",
    "    hp_df=hp_df.append(json_normalize(hps), ignore_index=True)\n",
    "    i+=1\n",
    "hp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters to get unique Pharmacy and Hospital Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_df=hp_df.drop_duplicates(['id'])\n",
    "hp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "#TODO figure out how to webscrape COVID-19 data from graph\n",
    "r = requests.get(\n",
    "    f\"https://public.tableau.com/views/COVID-19OutbreakSummary_15918845768300/COVID19Summaryp3\",\n",
    "     params= {\n",
    "        \":embed\":\"y\",\n",
    "        \":showAppBanner\":\"false\",\n",
    "        \":showShareOptions\":\"true\",\n",
    "        \":display_count\":\"no\",\n",
    "    }\n",
    ")\n",
    "print(r.url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "tableauData = json.loads(soup.find(\"textarea\",{\"id\": \"tsConfigContainer\"}).text)\n",
    "\n",
    "dataUrl = f'https://public.tableau.com{tableauData[\"vizql_root\"]}/bootstrapSession/sessions/{tableauData[\"sessionid\"]}'\n",
    "r = requests.post(dataUrl, data= {\n",
    "    \"sheet_id\": tableauData[\"sheetId\"],\n",
    "})\n",
    "dataReg = re.search('\\d+;({.*})\\d+;({.*})', r.text, re.MULTILINE)\n",
    "info = json.loads(dataReg.group(1))\n",
    "data = json.loads(dataReg.group(2))\n",
    "#print(info)\n",
    "print(data[\"secondaryInfo\"][\"presModelMap\"][\"dataDictionary\"][\"presModelHolder\"][\"genDataDictionaryPresModel\"][\"dataSegments\"][\"0\"][\"dataColumns\"])\n",
    "#<_sre.SRE_Match object; span=(0, 109884), match='72955;{\"sheetName\":\"DASH: Patients v discharges\",>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Testing Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "# source of pdf scraping https://www.youtube.com/watch?v=UmPe07a3bWs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes pdf itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "url = 'https://www.columbus.gov/uploadedFiles/Columbus/Departments/Public_Health/All_Programs/Emergency_Preparedness/Coronavirus/LocalTesting_FQHCs_7.16.2020.pdf'\n",
    "\n",
    "r = requests.get(url)\n",
    "f = io.BytesIO(r.content)\n",
    "\n",
    "reader = PdfFileReader(f)\n",
    "contents = reader.getPage(0).extractText().split('\\n')\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters to grab address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "address_extract ='\\d+[ ](?:[A-Za-z0-9.-]+[ ]?)+(?:Avenue|Lane|Road|Boulevard|Drive|Street|Ave|Dr|Rd|Blvd|Ln|St)\\.?'\n",
    "p = re.compile(address_extract)\n",
    "testingLocations = [ content for content in contents if p.match(content)]\n",
    "p2 = re.compile('\\d+\\.')\n",
    "covid_test_providers = [ content for content in contents if p2.match(content)]\n",
    "testingLocations = testingLocations+[re.sub(\"\\d+\\.\",\"\",test) for test in covid_test_providers if re.search(address_extract,test)]\n",
    "print(testingLocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome Selenium Webscraping Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import time\n",
    "# import Action chains  \n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Navigate to url\n",
    "driver.get(\"https://public.tableau.com/views/COVID-19OutbreakSummary_15918845768300/COVID19Summaryp3?%3Adisplay_count=y&%3Aorigin=viz_share_link&%3AshowVizHome=no\")\n",
    "\n",
    "# Store 'google search' button web element\n",
    "time.sleep(20)\n",
    "mapfre = driver.find_element_by_xpath(\"//*[@id=\\\"tabZoneId106\\\"]/div/div/div/div\")#//*[@id=\\\"view8239885986803340325_964909730081303188\\\"]/div[1]/div[2]\")\n",
    "#Set x and y offset positions of element\n",
    "xOffset = 0 \n",
    "yOffset = 0\n",
    "ActionChains(driver).move_to_element(mapfre).perform()\n",
    "ActionChains(driver).move_by_offset(xOffset,yOffset).perform()\n",
    "time.sleep(12)\n",
    "with open('temp_covid_zip.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(driver.page_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this script works for extracting zip and cases range\n",
    "temp = ''\n",
    "with open('temp_covid_zip.txt', 'r') as fin:\n",
    "    temp = fin.read()\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(temp, 'html.parser')\n",
    "table = soup.find_all('div')[len(soup.find_all('div'))-3].table #grabs the table which stores the data\n",
    "print(table.find_all('tr')[0].find_all('td')[2].string)\n",
    "print(table.find_all('tr')[1].find_all('td')[2].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use for Y: -39 to start off with to go from 43211 to 43224\n",
    "# Navigate to url\n",
    "driver.get(\"https://public.tableau.com/views/COVID-19OutbreakSummary_15918845768300/COVID19Summaryp3?%3Adisplay_count=y&%3Aorigin=viz_share_link&%3AshowVizHome=no\")\n",
    "\n",
    "# Store 'google search' button web element\n",
    "time.sleep(20)\n",
    "mapfre = driver.find_element_by_xpath(\"//*[@id=\\\"tabZoneId106\\\"]/div/div/div/div\")#//*[@id=\\\"view8239885986803340325_964909730081303188\\\"]/div[1]/div[2]\")\n",
    "#Set x and y offset positions of element\n",
    "xOffset = 0 \n",
    "yOffset = -39\n",
    "ActionChains(driver).move_to_element(mapfre).perform()\n",
    "ActionChains(driver).move_by_offset(xOffset,yOffset).perform()\n",
    "time.sleep(12)\n",
    "with open('temp_covid_zip.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(driver.page_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this script works for extracting zip and cases range\n",
    "# -39 is a success\n",
    "temp = ''\n",
    "with open('temp_covid_zip.txt', 'r') as fin:\n",
    "    temp = fin.read()\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(temp, 'html.parser')\n",
    "table = soup.find_all('div')[len(soup.find_all('div'))-3].table #grabs the table which stores the data\n",
    "print(table.find_all('tr')[0].find_all('td')[2].string)\n",
    "print(table.find_all('tr')[1].find_all('td')[2].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Calculate Difference between 43211 and 43224 latitude and create a scale factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
